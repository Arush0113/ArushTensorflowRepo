{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Coding Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9b3PNw3gJDI7",
        "hcPUtmz-JDKZ",
        "Zo5rD5ZcJDK_",
        "NrE0rpCVJDL1",
        "yrX43gwPJDL-",
        "MHcNqGnWJDMH",
        "9Ti4kMquJDML",
        "jR7y1e-xJDMd",
        "H3srEhqCJDM7"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arush0113/ArushTensorflowRepo/blob/main/Customising%20your%20models%20with%20Tensorflow%202/Coding_Tutorials_week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk88Aw4NJDIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51493df9-214b-4cea-e8aa-4b85d47fe7b1"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "print('GPU name: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "GPU name: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQb8nVqLJDI5"
      },
      "source": [
        "# Sequence modelling \n",
        "\n",
        "## Coding tutorials\n",
        " #### 1.  The IMDb dataset\n",
        " #### 2. Padding and masking sequence data\n",
        " #### 3. The `Embedding` layer\n",
        " #### 4. The Embedding Projector\n",
        " #### 5. Recurrent neural network layers\n",
        " #### 6. Stacked RNNs and the `Bidirectional` wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b3PNw3gJDI7"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## The IMDb Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwqGZQ8WJDJF"
      },
      "source": [
        "#### Load the IMDB review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3_c1LL5JDJG"
      },
      "source": [
        "# Import imdb\n",
        "\n",
        "import tensorflow.keras.datasets.imdb as imdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jYJCwXJDJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525f9a37-a448-4f98-e96f-7f07a12f18c2"
      },
      "source": [
        "# Download and assign the data set using load_data()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oCnB8UMJDJP"
      },
      "source": [
        "#### Inspect the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39RwIt-pJDJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9c0fd6-80ac-4070-bc14-6f81227c0f62"
      },
      "source": [
        "# Inspect the type of the data\n",
        "\n",
        "print(type(x_train))\n",
        "print(type(y_train))\n",
        "print(type(x_test))\n",
        "print(type(y_test))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Viryy_PDJDJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2338807-a517-41e3-df12-56cb4ae92f58"
      },
      "source": [
        "# Inspect the shape of the data\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q6E6v-FJDJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89e4cb06-b2ad-44f6-fa9f-2737185a5467"
      },
      "source": [
        "# Display the first dataset element input\n",
        "# Notice encoding\n",
        "\n",
        "x_train[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lbEqyjxJDJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80d350b-41ef-4ac1-9655-7eaf2d544106"
      },
      "source": [
        "# Display the first dataset element output\n",
        "\n",
        "y_train[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAk-PyOgJDJg"
      },
      "source": [
        "#### Load dataset with different options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfzFxDj-JDJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899c6208-aeda-4d80-835d-2ffb16105f13"
      },
      "source": [
        "# Load the dataset with defaults\n",
        "\n",
        "imdb.load_data(path = 'imdb.npz', index_from = 3)\n",
        "# ~/.keras/dataset/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "         ...,\n",
              "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
              "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
              " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
              "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
              "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
              "         ...,\n",
              "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
              "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
              "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
              "        dtype=object),\n",
              "  array([0, 1, 1, ..., 0, 0, 0])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuGfzZg3JDJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c07d323-37cd-44b5-a380-290187537ef2"
      },
      "source": [
        "# Limit the vocabulary to the top 500 words using num_words\n",
        "\n",
        "imdb.load_data(num_words=500)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([list([1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
              "         list([1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 2, 8, 118, 2, 14, 394, 20, 13, 119, 2, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 2, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 2, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 2, 7, 2, 2, 349, 2, 148, 2, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 2, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 2, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 2, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 2, 2, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 2, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 2, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 2, 116, 2, 2, 13, 191, 79, 2, 89, 2, 14, 9, 8, 106, 2, 2, 35, 2, 6, 227, 7, 129, 113]),\n",
              "         ...,\n",
              "         list([1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 2, 84, 2, 325, 2, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 2, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 2, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 2, 108, 45, 40, 29, 2, 395, 11, 6, 2, 2, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 2, 443, 2, 5, 27, 2, 117, 2, 2, 165, 47, 84, 37, 131, 2, 14, 2, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 2, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 2, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 2, 2, 372, 2, 2, 2, 2, 7, 4, 59, 2, 4, 2, 2]),\n",
              "         list([1, 2, 2, 69, 72, 2, 13, 2, 2, 8, 12, 2, 23, 5, 16, 484, 2, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 2, 32, 61, 369, 71, 66, 2, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 2, 75, 2, 44, 257, 390, 5, 69, 263, 2, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 2, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 2, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 2, 25, 8, 2, 12, 145, 5, 202, 12, 160, 2, 202, 12, 6, 52, 58, 2, 92, 401, 2, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 2, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 2, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 2, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 2, 5, 383, 2, 2, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 2, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9])],\n",
              "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
              " (array([list([1, 2, 202, 14, 31, 6, 2, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 2]),\n",
              "         list([1, 14, 22, 2, 6, 176, 7, 2, 88, 12, 2, 23, 2, 5, 109, 2, 4, 114, 9, 55, 2, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2, 109, 2, 21, 4, 22, 2, 8, 6, 2, 2, 10, 10, 4, 105, 2, 35, 2, 2, 19, 2, 2, 5, 2, 2, 45, 55, 221, 15, 2, 2, 2, 14, 2, 4, 405, 5, 2, 7, 27, 85, 108, 131, 4, 2, 2, 2, 405, 9, 2, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 2, 239, 34, 2, 2, 45, 407, 31, 7, 41, 2, 105, 21, 59, 299, 12, 38, 2, 5, 2, 15, 45, 2, 488, 2, 127, 6, 52, 292, 17, 4, 2, 185, 132, 2, 2, 2, 488, 2, 47, 6, 392, 173, 4, 2, 2, 270, 2, 4, 2, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 2, 2, 7, 2, 2, 2, 5, 2, 30, 2, 2, 56, 4, 2, 5, 2, 2, 8, 4, 2, 398, 229, 10, 10, 13, 2, 2, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2, 19, 2, 2, 2, 2, 14, 22, 9, 2, 21, 45, 2, 5, 45, 252, 8, 2, 6, 2, 2, 2, 39, 4, 2, 48, 25, 181, 8, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 2, 8, 169, 11, 374, 2, 25, 203, 28, 8, 2, 12, 125, 4, 2]),\n",
              "         list([1, 111, 2, 2, 2, 2, 2, 4, 87, 2, 2, 7, 31, 318, 2, 7, 4, 498, 2, 2, 63, 29, 2, 220, 2, 2, 5, 17, 12, 2, 220, 2, 17, 6, 185, 132, 2, 16, 53, 2, 11, 2, 74, 4, 438, 21, 27, 2, 2, 8, 22, 107, 2, 2, 2, 2, 8, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 5, 2, 98, 31, 2, 33, 6, 58, 14, 2, 2, 8, 4, 365, 7, 2, 2, 356, 346, 4, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 2, 431, 2, 7, 32, 2, 16, 11, 94, 2, 10, 10, 4, 2, 2, 7, 4, 2, 2, 2, 2, 8, 2, 8, 2, 121, 31, 7, 27, 86, 2, 2, 16, 6, 465, 2, 2, 2, 2, 17, 2, 42, 4, 2, 37, 473, 6, 2, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 2, 2, 53, 33, 2, 2, 37, 70, 2, 4, 2, 2, 74, 476, 37, 62, 91, 2, 169, 4, 2, 2, 146, 2, 2, 5, 258, 12, 184, 2, 2, 5, 2, 2, 7, 4, 22, 2, 18, 2, 2, 2, 7, 4, 2, 71, 348, 425, 2, 2, 19, 2, 5, 2, 11, 2, 8, 339, 2, 4, 2, 2, 7, 4, 2, 10, 10, 263, 2, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 2, 19, 68, 2, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 2, 2, 36, 2, 8, 2, 2, 18, 6, 2, 4, 2, 26, 2, 2, 11, 14, 2, 2, 12, 426, 28, 77, 2, 8, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 9, 2, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2, 247, 30, 2, 6, 2, 54, 2, 2, 98, 6, 2, 40, 2, 37, 2, 98, 4, 2, 2, 15, 14, 9, 57, 2, 5, 2, 6, 275, 2, 2, 2, 2, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 2, 37, 2, 2, 98, 4, 2, 2, 90, 19, 6, 2, 7, 2, 2, 2, 4, 2, 2, 2, 8, 2, 90, 4, 2, 8, 4, 2, 17, 2, 2, 2, 4, 2, 8, 2, 189, 4, 2, 2, 2, 4, 2, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 2, 6, 425, 2, 2, 2, 2, 7, 4, 2, 2, 469, 4, 2, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 5, 2, 68, 2, 19, 2, 2, 4, 2, 7, 263, 65, 2, 34, 6, 2, 2, 43, 159, 29, 9, 2, 9, 387, 73, 195, 2, 10, 10, 2, 4, 58, 2, 54, 14, 2, 117, 22, 16, 93, 5, 2, 4, 192, 15, 12, 16, 93, 34, 6, 2, 2, 33, 4, 2, 7, 15, 2, 2, 2, 325, 12, 62, 30, 2, 8, 67, 14, 17, 6, 2, 44, 148, 2, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 2, 2, 27, 2, 7, 2, 4, 22, 2, 17, 6, 2, 2, 7, 2, 2, 2, 100, 30, 4, 2, 2, 2, 2, 42, 2, 11, 4, 2, 42, 101, 2, 7, 101, 2, 15, 2, 94, 2, 180, 5, 9, 2, 34, 2, 45, 6, 2, 22, 60, 6, 2, 31, 11, 94, 2, 96, 21, 94, 2, 9, 57, 2]),\n",
              "         ...,\n",
              "         list([1, 13, 2, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 2, 2, 2, 2, 2, 395, 2, 5, 2, 11, 119, 2, 89, 2, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2, 284, 2, 2, 37, 315, 4, 226, 20, 272, 2, 40, 29, 152, 60, 181, 8, 30, 50, 2, 362, 80, 119, 12, 21, 2, 2]),\n",
              "         list([1, 11, 119, 241, 9, 4, 2, 20, 12, 468, 15, 94, 2, 2, 2, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2, 7, 2, 46, 2, 9, 2, 5, 4, 2, 47, 8, 79, 90, 145, 164, 162, 50, 6, 2, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 2, 200, 5, 2, 5, 9, 2, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 2, 13, 2, 14, 20, 6, 2, 7, 470]),\n",
              "         list([1, 6, 52, 2, 430, 22, 9, 220, 2, 8, 28, 2, 2, 2, 6, 2, 15, 47, 6, 2, 2, 8, 114, 5, 33, 222, 31, 55, 184, 2, 2, 2, 19, 346, 2, 5, 6, 364, 350, 4, 184, 2, 9, 133, 2, 11, 2, 2, 21, 4, 2, 2, 2, 50, 2, 2, 9, 6, 2, 17, 6, 2, 2, 21, 17, 6, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 2, 19, 4, 78, 173, 7, 27, 2, 2, 2, 2, 2, 9, 6, 2, 17, 210, 5, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 2, 7, 4, 314, 74, 6, 2, 22, 2, 19, 2, 2, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 2, 2, 2, 4, 2, 25, 124, 4, 31, 12, 16, 93, 2, 34, 2, 2])],\n",
              "        dtype=object),\n",
              "  array([0, 1, 1, ..., 0, 0, 0])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMkXgUGaJDJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31817dba-2844-4d2d-895a-85cb4f3692ac"
      },
      "source": [
        "# Ignore the top 10 most frequent words using skip_top\n",
        "\n",
        "imdb.load_data(skip_top = 10, num_words=1000, oov_char=2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([list([2, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 2, 173, 36, 256, 2, 25, 100, 43, 838, 112, 50, 670, 2, 2, 35, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 39, 2, 172, 2, 2, 17, 546, 38, 13, 447, 2, 192, 50, 16, 2, 147, 2, 19, 14, 22, 2, 2, 2, 469, 2, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 2, 22, 17, 515, 17, 12, 16, 626, 18, 2, 2, 62, 386, 12, 2, 316, 2, 106, 2, 2, 2, 2, 16, 480, 66, 2, 33, 2, 130, 12, 16, 38, 619, 2, 25, 124, 51, 36, 135, 48, 25, 2, 33, 2, 22, 12, 215, 28, 77, 52, 2, 14, 407, 16, 82, 2, 2, 2, 107, 117, 2, 15, 256, 2, 2, 2, 2, 2, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 2, 2, 2, 2, 13, 104, 88, 2, 381, 15, 297, 98, 32, 2, 56, 26, 141, 2, 194, 2, 18, 2, 226, 22, 21, 134, 476, 26, 480, 2, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 2, 226, 65, 16, 38, 2, 88, 12, 16, 283, 2, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
              "         list([2, 194, 2, 194, 2, 78, 228, 2, 2, 2, 2, 2, 134, 26, 2, 715, 2, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 2, 207, 110, 2, 21, 14, 69, 188, 2, 30, 23, 2, 2, 249, 126, 93, 2, 114, 2, 2, 2, 2, 647, 2, 116, 2, 35, 2, 2, 229, 2, 340, 2, 2, 118, 2, 2, 130, 2, 19, 2, 2, 2, 89, 29, 952, 46, 37, 2, 455, 2, 45, 43, 38, 2, 2, 398, 2, 2, 26, 2, 2, 163, 11, 2, 2, 2, 2, 2, 194, 775, 2, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 2, 2, 228, 2, 43, 2, 2, 15, 299, 120, 2, 120, 174, 11, 220, 175, 136, 50, 2, 2, 228, 2, 2, 2, 656, 245, 2, 2, 2, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 2, 2, 371, 78, 22, 625, 64, 2, 2, 2, 168, 145, 23, 2, 2, 15, 16, 2, 2, 2, 28, 2, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "         list([2, 14, 47, 2, 30, 31, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 2, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 2, 86, 320, 35, 534, 19, 263, 2, 2, 2, 2, 33, 89, 78, 12, 66, 16, 2, 360, 2, 2, 58, 316, 334, 11, 2, 2, 43, 645, 662, 2, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 2, 106, 14, 2, 2, 18, 2, 22, 12, 215, 28, 610, 40, 2, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 2, 22, 47, 2, 2, 51, 2, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 2, 2, 106, 607, 624, 35, 534, 2, 227, 2, 129, 113]),\n",
              "         ...,\n",
              "         list([2, 11, 2, 230, 245, 2, 2, 2, 2, 446, 2, 45, 2, 84, 2, 2, 21, 2, 912, 84, 2, 325, 725, 134, 2, 2, 84, 2, 36, 28, 57, 2, 21, 2, 140, 2, 703, 2, 2, 84, 56, 18, 2, 14, 2, 31, 2, 2, 2, 2, 2, 2, 2, 18, 2, 20, 207, 110, 563, 12, 2, 2, 2, 2, 97, 2, 20, 53, 2, 74, 2, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 2, 2, 500, 2, 2, 89, 364, 70, 29, 140, 2, 64, 2, 11, 2, 2, 26, 178, 2, 529, 443, 2, 2, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 2, 65, 496, 2, 231, 2, 790, 2, 2, 320, 234, 2, 234, 2, 2, 2, 496, 2, 139, 929, 2, 2, 2, 2, 2, 18, 2, 2, 2, 250, 11, 2, 2, 2, 2, 2, 747, 2, 372, 2, 2, 541, 2, 2, 2, 59, 2, 2, 2, 2]),\n",
              "         list([2, 2, 2, 69, 72, 2, 13, 610, 930, 2, 12, 582, 23, 2, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 2, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 2, 2, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 2, 69, 263, 514, 105, 50, 286, 2, 23, 2, 123, 13, 161, 40, 2, 421, 2, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 2, 719, 2, 13, 18, 31, 62, 40, 2, 2, 2, 2, 2, 14, 123, 2, 942, 25, 2, 721, 12, 145, 2, 202, 12, 160, 580, 202, 12, 2, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 2, 15, 251, 2, 2, 12, 38, 84, 80, 124, 12, 2, 23]),\n",
              "         list([2, 17, 2, 194, 337, 2, 2, 204, 22, 45, 254, 2, 106, 14, 123, 2, 2, 270, 2, 2, 2, 2, 732, 2, 101, 405, 39, 14, 2, 2, 2, 2, 115, 50, 305, 12, 47, 2, 168, 2, 235, 2, 38, 111, 699, 102, 2, 2, 2, 2, 2, 24, 2, 78, 2, 17, 2, 2, 21, 27, 2, 2, 2, 2, 2, 92, 2, 2, 2, 2, 2, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 2, 97, 12, 157, 21, 2, 2, 2, 2, 66, 78, 2, 2, 631, 2, 2, 2, 272, 191, 2, 2, 2, 2, 2, 2, 2, 544, 2, 383, 2, 848, 2, 2, 497, 2, 2, 2, 2, 2, 21, 60, 27, 239, 2, 43, 2, 209, 405, 10, 10, 12, 764, 40, 2, 248, 20, 12, 16, 2, 174, 2, 72, 2, 51, 2, 2, 22, 2, 204, 131, 2])],\n",
              "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
              " (array([list([2, 591, 202, 14, 31, 2, 717, 10, 10, 2, 2, 2, 2, 360, 2, 2, 177, 2, 394, 354, 2, 123, 2, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 2, 124, 14, 286, 170, 2, 157, 46, 2, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 2, 717]),\n",
              "         list([2, 14, 22, 2, 2, 176, 2, 2, 88, 12, 2, 23, 2, 2, 109, 943, 2, 114, 2, 55, 606, 2, 111, 2, 2, 139, 193, 273, 23, 2, 172, 270, 11, 2, 2, 2, 2, 2, 109, 2, 21, 2, 22, 2, 2, 2, 2, 2, 10, 10, 2, 105, 987, 35, 841, 2, 19, 861, 2, 2, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 2, 405, 2, 2, 2, 27, 85, 108, 131, 2, 2, 2, 2, 405, 2, 2, 133, 2, 50, 13, 104, 51, 66, 166, 14, 22, 157, 2, 2, 530, 239, 34, 2, 2, 45, 407, 31, 2, 41, 2, 105, 21, 59, 299, 12, 38, 950, 2, 2, 15, 45, 629, 488, 2, 127, 2, 52, 292, 17, 2, 2, 185, 132, 2, 2, 2, 488, 2, 47, 2, 392, 173, 2, 2, 2, 270, 2, 2, 2, 2, 2, 65, 55, 73, 11, 346, 14, 20, 2, 2, 976, 2, 2, 2, 861, 2, 2, 2, 30, 2, 2, 56, 2, 841, 2, 990, 692, 2, 2, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 2, 31, 2, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 2, 2, 21, 45, 2, 2, 45, 252, 2, 2, 2, 565, 921, 2, 39, 2, 529, 48, 25, 181, 2, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 2, 290, 2, 58, 10, 10, 472, 45, 55, 878, 2, 169, 11, 374, 2, 25, 203, 28, 2, 818, 12, 125, 2, 2]),\n",
              "         list([2, 111, 748, 2, 2, 2, 2, 2, 87, 2, 2, 2, 31, 318, 2, 2, 2, 498, 2, 748, 63, 29, 2, 220, 686, 2, 2, 17, 12, 575, 220, 2, 17, 2, 185, 132, 2, 16, 53, 928, 11, 2, 74, 2, 438, 21, 27, 2, 589, 2, 22, 107, 2, 2, 997, 2, 2, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 2, 2, 98, 31, 2, 33, 2, 58, 14, 2, 2, 2, 2, 365, 2, 2, 2, 356, 346, 2, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 2, 58, 54, 2, 431, 748, 2, 32, 2, 16, 11, 94, 2, 10, 10, 2, 993, 2, 2, 2, 2, 2, 2, 2, 2, 847, 2, 2, 121, 31, 2, 27, 86, 2, 2, 16, 2, 465, 993, 2, 2, 573, 17, 2, 42, 2, 2, 37, 473, 2, 711, 2, 2, 2, 328, 212, 70, 30, 258, 11, 220, 32, 2, 108, 21, 133, 12, 2, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 2, 2, 2, 74, 476, 37, 62, 91, 2, 169, 2, 2, 2, 146, 655, 2, 2, 258, 12, 184, 2, 546, 2, 849, 2, 2, 2, 22, 2, 18, 631, 2, 797, 2, 2, 2, 71, 348, 425, 2, 2, 19, 2, 2, 2, 11, 661, 2, 339, 2, 2, 2, 2, 2, 2, 2, 10, 10, 263, 787, 2, 270, 11, 2, 2, 2, 2, 2, 121, 2, 2, 26, 2, 19, 68, 2, 2, 28, 446, 2, 318, 2, 2, 67, 51, 36, 70, 81, 2, 2, 2, 36, 2, 2, 2, 2, 18, 2, 711, 2, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 2, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 2, 2, 17, 2, 2, 428, 2, 232, 11, 2, 2, 37, 272, 40, 2, 247, 30, 656, 2, 2, 54, 2, 2, 98, 2, 2, 40, 558, 37, 2, 98, 2, 2, 2, 15, 14, 2, 57, 2, 2, 2, 2, 275, 711, 2, 2, 2, 98, 2, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 2, 2, 2, 90, 19, 2, 2, 2, 2, 2, 2, 2, 2, 2, 930, 2, 508, 90, 2, 2, 2, 2, 2, 17, 2, 2, 2, 2, 2, 2, 2, 189, 2, 2, 2, 2, 2, 2, 2, 95, 271, 23, 2, 2, 2, 2, 2, 33, 2, 2, 425, 2, 2, 2, 2, 2, 2, 2, 2, 469, 2, 2, 54, 2, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 2, 2, 68, 2, 19, 2, 2, 2, 2, 2, 263, 65, 2, 34, 2, 2, 2, 43, 159, 29, 2, 2, 2, 387, 73, 195, 584, 10, 10, 2, 2, 58, 810, 54, 14, 2, 117, 22, 16, 93, 2, 2, 2, 192, 15, 12, 16, 93, 34, 2, 2, 2, 33, 2, 2, 2, 15, 2, 2, 2, 325, 12, 62, 30, 776, 2, 67, 14, 17, 2, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 2, 819, 2, 22, 2, 17, 2, 2, 787, 2, 2, 2, 2, 100, 30, 2, 2, 2, 2, 2, 42, 2, 11, 2, 2, 42, 101, 704, 2, 101, 999, 15, 2, 94, 2, 180, 2, 2, 2, 34, 2, 45, 2, 2, 22, 60, 2, 2, 31, 11, 94, 2, 96, 21, 94, 749, 2, 57, 975]),\n",
              "         ...,\n",
              "         list([2, 13, 2, 15, 2, 135, 14, 2, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 2, 2, 910, 769, 2, 2, 395, 2, 2, 2, 11, 119, 2, 89, 2, 2, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 2, 185, 2, 284, 2, 2, 37, 315, 2, 226, 20, 272, 2, 40, 29, 152, 60, 181, 2, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
              "         list([2, 11, 119, 241, 2, 2, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 2, 86, 107, 2, 97, 14, 31, 33, 2, 2, 2, 743, 46, 2, 2, 2, 2, 2, 768, 47, 2, 79, 90, 145, 164, 162, 50, 2, 501, 119, 2, 2, 2, 78, 232, 15, 16, 224, 11, 2, 333, 20, 2, 985, 200, 2, 2, 2, 2, 2, 2, 79, 357, 2, 20, 47, 220, 57, 206, 139, 11, 12, 2, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 2, 2, 2, 470]),\n",
              "         list([2, 2, 52, 2, 430, 22, 2, 220, 2, 2, 28, 2, 519, 2, 2, 769, 15, 47, 2, 2, 2, 2, 114, 2, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 2, 2, 364, 350, 2, 184, 2, 2, 133, 2, 11, 2, 2, 21, 2, 2, 2, 570, 50, 2, 2, 2, 2, 2, 17, 2, 2, 2, 21, 17, 2, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 2, 194, 21, 29, 218, 2, 19, 2, 78, 173, 2, 27, 2, 2, 2, 718, 2, 2, 2, 2, 17, 210, 2, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 2, 392, 217, 21, 50, 2, 57, 65, 12, 2, 53, 40, 35, 390, 2, 11, 2, 2, 2, 2, 314, 74, 2, 792, 22, 2, 19, 714, 727, 2, 382, 2, 91, 2, 439, 19, 14, 20, 2, 2, 2, 2, 2, 756, 25, 124, 2, 31, 12, 16, 93, 804, 34, 2, 2])],\n",
              "        dtype=object),\n",
              "  array([0, 1, 1, ..., 0, 0, 0])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBDAFt0FJDJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d698f2bd-0a67-4c2b-9f94-f0a2dbd97a74"
      },
      "source": [
        "# Limit the sequence lengths to 500 using maxlen\n",
        "\n",
        "imdb.load_data(maxlen=500)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "         ...,\n",
              "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
              "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
              " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
              "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
              "         list([1, 13, 1228, 119, 14, 552, 7, 20, 190, 14, 58, 13, 258, 546, 1786, 8, 1968, 4, 268, 237, 13, 191, 81, 15, 13, 80, 43, 3824, 44, 12, 14, 16, 427, 3192, 4, 183, 15, 593, 19, 4, 351, 362, 26, 55, 646, 21, 4, 1239, 84, 26, 1557, 3755, 13, 244, 6, 2071, 132, 184, 194, 5, 13, 70, 4478, 546, 73, 190, 13, 62, 24, 81, 320, 4, 538, 4, 117, 250, 127, 11, 14, 20, 82, 4, 452, 11, 14, 20, 9, 8654, 19, 41, 476, 8, 4, 213, 7, 9185, 13, 657, 13, 286, 38, 1612, 44, 41, 5, 41, 1729, 88, 13, 62, 28, 900, 510, 4, 509, 51, 6, 612, 59, 16, 193, 61, 4666, 5, 702, 930, 143, 285, 25, 67, 41, 81, 366, 4, 130, 82, 9, 259, 334, 397, 1195, 7, 149, 102, 15, 26, 814, 38, 465, 1627, 31, 70, 983, 67, 51, 9, 112, 814, 17, 35, 311, 75, 26, 11649, 574, 19, 4, 1729, 23, 4, 268, 38, 95, 138, 4, 609, 191, 75, 28, 314, 1772]),\n",
              "         ...,\n",
              "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
              "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
              "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
              "        dtype=object),\n",
              "  array([0, 1, 0, ..., 0, 0, 0])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI-Nr897LorW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f168fb4f-97c3-413d-e064-5476078613e8"
      },
      "source": [
        " # Use '1' as the character that indicates the start of a sequence\n",
        "\n",
        "imdb.load_data(start_char=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "         ...,\n",
              "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
              "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
              " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
              "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
              "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
              "         ...,\n",
              "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
              "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
              "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
              "        dtype=object),\n",
              "  array([0, 1, 1, ..., 0, 0, 0])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuBCKKGyJDJ7"
      },
      "source": [
        "#### Explore the dataset word index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gkaFf6MJDJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8eb073-b3b5-418d-fb3e-fda7b5b80c8a"
      },
      "source": [
        "# Load the imdb word index using get_word_index()\n",
        "\n",
        "imdb_word_index = imdb.get_word_index()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUzgAIoKJDKB"
      },
      "source": [
        "# View the word index as a dictionary,\n",
        "# accounting for index_from.\n",
        "\n",
        "index_from = 3\n",
        "imdb_word_index = {key: val + index_from for key, val in imdb_word_index.items()}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huCi_QIzJDKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24bf19a5-ab18-4c72-bb6e-947457bb3ea4"
      },
      "source": [
        "# Retrieve a specific word's index\n",
        "\n",
        "imdb_word_index['simpsonian']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9X4OO_FNGZx",
        "outputId": "0a3c6b50-3b37-4b4b-9cd5-89504a393187"
      },
      "source": [
        "imdb_word_index['the']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7UkZwHiJDKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4705a6-dca8-4e41-83b3-2f546e7e238c"
      },
      "source": [
        "# View an input sentence\n",
        "\n",
        "inv_imdb_word_index = {val: key for key, val in imdb_word_index.items()}\n",
        "[inv_imdb_word_index[index] for index in x_train[0] if index > index_from]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'film',\n",
              " 'was',\n",
              " 'just',\n",
              " 'brilliant',\n",
              " 'casting',\n",
              " 'location',\n",
              " 'scenery',\n",
              " 'story',\n",
              " 'direction',\n",
              " \"everyone's\",\n",
              " 'really',\n",
              " 'suited',\n",
              " 'the',\n",
              " 'part',\n",
              " 'they',\n",
              " 'played',\n",
              " 'and',\n",
              " 'you',\n",
              " 'could',\n",
              " 'just',\n",
              " 'imagine',\n",
              " 'being',\n",
              " 'there',\n",
              " 'robert',\n",
              " \"redford's\",\n",
              " 'is',\n",
              " 'an',\n",
              " 'amazing',\n",
              " 'actor',\n",
              " 'and',\n",
              " 'now',\n",
              " 'the',\n",
              " 'same',\n",
              " 'being',\n",
              " 'director',\n",
              " \"norman's\",\n",
              " 'father',\n",
              " 'came',\n",
              " 'from',\n",
              " 'the',\n",
              " 'same',\n",
              " 'scottish',\n",
              " 'island',\n",
              " 'as',\n",
              " 'myself',\n",
              " 'so',\n",
              " 'i',\n",
              " 'loved',\n",
              " 'the',\n",
              " 'fact',\n",
              " 'there',\n",
              " 'was',\n",
              " 'a',\n",
              " 'real',\n",
              " 'connection',\n",
              " 'with',\n",
              " 'this',\n",
              " 'film',\n",
              " 'the',\n",
              " 'witty',\n",
              " 'remarks',\n",
              " 'throughout',\n",
              " 'the',\n",
              " 'film',\n",
              " 'were',\n",
              " 'great',\n",
              " 'it',\n",
              " 'was',\n",
              " 'just',\n",
              " 'brilliant',\n",
              " 'so',\n",
              " 'much',\n",
              " 'that',\n",
              " 'i',\n",
              " 'bought',\n",
              " 'the',\n",
              " 'film',\n",
              " 'as',\n",
              " 'soon',\n",
              " 'as',\n",
              " 'it',\n",
              " 'was',\n",
              " 'released',\n",
              " 'for',\n",
              " 'retail',\n",
              " 'and',\n",
              " 'would',\n",
              " 'recommend',\n",
              " 'it',\n",
              " 'to',\n",
              " 'everyone',\n",
              " 'to',\n",
              " 'watch',\n",
              " 'and',\n",
              " 'the',\n",
              " 'fly',\n",
              " 'fishing',\n",
              " 'was',\n",
              " 'amazing',\n",
              " 'really',\n",
              " 'cried',\n",
              " 'at',\n",
              " 'the',\n",
              " 'end',\n",
              " 'it',\n",
              " 'was',\n",
              " 'so',\n",
              " 'sad',\n",
              " 'and',\n",
              " 'you',\n",
              " 'know',\n",
              " 'what',\n",
              " 'they',\n",
              " 'say',\n",
              " 'if',\n",
              " 'you',\n",
              " 'cry',\n",
              " 'at',\n",
              " 'a',\n",
              " 'film',\n",
              " 'it',\n",
              " 'must',\n",
              " 'have',\n",
              " 'been',\n",
              " 'good',\n",
              " 'and',\n",
              " 'this',\n",
              " 'definitely',\n",
              " 'was',\n",
              " 'also',\n",
              " 'congratulations',\n",
              " 'to',\n",
              " 'the',\n",
              " 'two',\n",
              " 'little',\n",
              " \"boy's\",\n",
              " 'that',\n",
              " 'played',\n",
              " 'the',\n",
              " \"part's\",\n",
              " 'of',\n",
              " 'norman',\n",
              " 'and',\n",
              " 'paul',\n",
              " 'they',\n",
              " 'were',\n",
              " 'just',\n",
              " 'brilliant',\n",
              " 'children',\n",
              " 'are',\n",
              " 'often',\n",
              " 'left',\n",
              " 'out',\n",
              " 'of',\n",
              " 'the',\n",
              " 'praising',\n",
              " 'list',\n",
              " 'i',\n",
              " 'think',\n",
              " 'because',\n",
              " 'the',\n",
              " 'stars',\n",
              " 'that',\n",
              " 'play',\n",
              " 'them',\n",
              " 'all',\n",
              " 'grown',\n",
              " 'up',\n",
              " 'are',\n",
              " 'such',\n",
              " 'a',\n",
              " 'big',\n",
              " 'profile',\n",
              " 'for',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'film',\n",
              " 'but',\n",
              " 'these',\n",
              " 'children',\n",
              " 'are',\n",
              " 'amazing',\n",
              " 'and',\n",
              " 'should',\n",
              " 'be',\n",
              " 'praised',\n",
              " 'for',\n",
              " 'what',\n",
              " 'they',\n",
              " 'have',\n",
              " 'done',\n",
              " \"don't\",\n",
              " 'you',\n",
              " 'think',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'story',\n",
              " 'was',\n",
              " 'so',\n",
              " 'lovely',\n",
              " 'because',\n",
              " 'it',\n",
              " 'was',\n",
              " 'true',\n",
              " 'and',\n",
              " 'was',\n",
              " \"someone's\",\n",
              " 'life',\n",
              " 'after',\n",
              " 'all',\n",
              " 'that',\n",
              " 'was',\n",
              " 'shared',\n",
              " 'with',\n",
              " 'us',\n",
              " 'all']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq2ID8wRJDKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c0152f-e5db-4637-8903-20059c01fe02"
      },
      "source": [
        "# Get the sentiment value\n",
        "\n",
        "y_train[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcPUtmz-JDKZ"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Padding and Masking Sequence Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UggNd-8VLgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8555ff-ffe7-4074-b490-b5ac381beaa5"
      },
      "source": [
        "# Load the imdb data set\n",
        "\n",
        "import tensorflow.keras.datasets.imdb as imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuqWhXi-JDKa"
      },
      "source": [
        "#### Preprocess the data with padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWULtJ7CJDKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72284f8-f045-459d-d15e-303f96efb90c"
      },
      "source": [
        "# Inspect the input data shape\n",
        "\n",
        "x_train.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOts_k01JDKh"
      },
      "source": [
        "# Pad the inputs to the maximum length using maxlen\n",
        "\n",
        "padded_x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=300, padding='post', truncating='pre')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNPiMGwDJDKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e07b4cb-f5f4-44b7-a14e-c604275893c8"
      },
      "source": [
        "# Inspect the output data shape\n",
        "\n",
        "padded_x_train.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJt56letJDKn"
      },
      "source": [
        "#### Create a Masking layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdoMdifYJDKo"
      },
      "source": [
        "# Import numpy \n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8LjX9QaJDKr"
      },
      "source": [
        "# Masking expects to see (batch, sequence, features)\n",
        "# Create a dummy feature dimension using expand_dims\n",
        "\n",
        "padded_x_train = np.expand_dims(padded_x_train, -1)\n",
        "#OR USE padded_x_train = padded_x_train[..., np.newaxis]\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFrgXbDrJDKt"
      },
      "source": [
        "# Create a Masking layer \n",
        "\n",
        "tf_x_train = tf.convert_to_tensor(padded_x_train, dtype='float32')\n",
        "masking_layer = tf.keras.layers.Masking(mask_value=0.0)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kkSzdHwJDKw"
      },
      "source": [
        "# Pass tf_x_train to it\n",
        "\n",
        "masked_x_train = masking_layer(tf_x_train)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuStn9s0JDK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2be88a-224d-4b19-c851-5f09a658a8c9"
      },
      "source": [
        "# Look at the dataset\n",
        "\n",
        "tf_x_train"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
              "array([[[1.000e+00],\n",
              "        [1.400e+01],\n",
              "        [2.200e+01],\n",
              "        ...,\n",
              "        [0.000e+00],\n",
              "        [0.000e+00],\n",
              "        [0.000e+00]],\n",
              "\n",
              "       [[1.000e+00],\n",
              "        [1.940e+02],\n",
              "        [1.153e+03],\n",
              "        ...,\n",
              "        [0.000e+00],\n",
              "        [0.000e+00],\n",
              "        [0.000e+00]],\n",
              "\n",
              "       [[1.000e+00],\n",
              "        [1.400e+01],\n",
              "        [4.700e+01],\n",
              "        ...,\n",
              "        [0.000e+00],\n",
              "        [0.000e+00],\n",
              "        [0.000e+00]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[1.000e+00],\n",
              "        [1.100e+01],\n",
              "        [6.000e+00],\n",
              "        ...,\n",
              "        [0.000e+00],\n",
              "        [0.000e+00],\n",
              "        [0.000e+00]],\n",
              "\n",
              "       [[1.000e+00],\n",
              "        [1.446e+03],\n",
              "        [7.079e+03],\n",
              "        ...,\n",
              "        [0.000e+00],\n",
              "        [0.000e+00],\n",
              "        [0.000e+00]],\n",
              "\n",
              "       [[1.000e+00],\n",
              "        [1.700e+01],\n",
              "        [6.000e+00],\n",
              "        ...,\n",
              "        [0.000e+00],\n",
              "        [0.000e+00],\n",
              "        [0.000e+00]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "9GYG-UlOWJso",
        "outputId": "07cb09ef-02bf-4b8a-c5c4-8330a24fc674"
      },
      "source": [
        "tf_x_train._keras_mask"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-d1eb65989b0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_x_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O54DVLx4JDK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f3335a-b61a-4398-adb7-1613bde7bd53"
      },
      "source": [
        "# Look at the ._keras_mask for the dataset\n",
        "\n",
        "masked_x_train._keras_mask"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(25000, 300), dtype=bool, numpy=\n",
              "array([[ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       ...,\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo5rD5ZcJDK_"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## The Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCkBOM8mJDLA"
      },
      "source": [
        "#### Create and apply an `Embedding` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-esPcdDJDLJ"
      },
      "source": [
        "# Create an embedding layer using layers.Embedding\n",
        "# Specify input_dim, output_dim, input_length\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim = 501, output_dim = 16)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf3B6HamJDLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c85d7db-7a28-40d7-dd6f-1d5b20becbac"
      },
      "source": [
        "# Inspect an Embedding layer output for a fixed input\n",
        "# Expects an input of shape (batch, sequence, feature)\n",
        "\n",
        "sequence_of_indices = tf.constant([[[0], [1], [5], [500]]])\n",
        "sequence_of_embeddings = embedding_layer(sequence_of_indices)\n",
        "sequence_of_embeddings"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4, 1, 16), dtype=float32, numpy=\n",
              "array([[[[ 0.00801451,  0.01410736,  0.00116625, -0.01825025,\n",
              "          -0.0384746 ,  0.00292188, -0.0087936 ,  0.01050808,\n",
              "           0.04017753, -0.00575219,  0.02110498, -0.00291384,\n",
              "          -0.03358424,  0.01205326, -0.00746052, -0.03850379]],\n",
              "\n",
              "        [[-0.00414885,  0.04440394,  0.03385458, -0.0151058 ,\n",
              "          -0.01384783, -0.02079234,  0.02053037, -0.02441523,\n",
              "           0.04315896,  0.00203233,  0.01651395, -0.0214765 ,\n",
              "          -0.02852841,  0.03200163, -0.03477192,  0.03553791]],\n",
              "\n",
              "        [[ 0.0348263 ,  0.04035548, -0.00569675,  0.03552583,\n",
              "           0.02546612, -0.00661119,  0.03668726, -0.02101131,\n",
              "           0.01074057,  0.0151335 , -0.01229682, -0.0384752 ,\n",
              "           0.02322042, -0.01271882, -0.02742435, -0.03294791]],\n",
              "\n",
              "        [[-0.02166654,  0.0129908 ,  0.03115931,  0.01896831,\n",
              "           0.02205174, -0.00571315,  0.04875709,  0.02377924,\n",
              "           0.04006238,  0.04366055, -0.04603047,  0.01078273,\n",
              "          -0.04021131,  0.04317986,  0.04155289, -0.00297077]]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ualmsaPpJDLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75972205-6964-400c-d0bd-f6db0caa0d60"
      },
      "source": [
        "# Inspect the Embedding layer weights using get_weights()\n",
        "\n",
        "embedding_layer.get_weights()[0]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00801451,  0.01410736,  0.00116625, ...,  0.01205326,\n",
              "        -0.00746052, -0.03850379],\n",
              "       [-0.00414885,  0.04440394,  0.03385458, ...,  0.03200163,\n",
              "        -0.03477192,  0.03553791],\n",
              "       [-0.04892945,  0.01380393,  0.01735872, ..., -0.00218714,\n",
              "        -0.02464647,  0.03643348],\n",
              "       ...,\n",
              "       [-0.02479465, -0.01801135, -0.04994736, ...,  0.01141549,\n",
              "        -0.02161474,  0.02225964],\n",
              "       [ 0.01950515,  0.02048507, -0.04489483, ..., -0.03850013,\n",
              "         0.01109854,  0.02306087],\n",
              "       [-0.02166654,  0.0129908 ,  0.03115931, ...,  0.04317986,\n",
              "         0.04155289, -0.00297077]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wadlt3AJDLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fbea81-7728-4829-feb4-9b9c7de1618d"
      },
      "source": [
        "# Get the embedding for the 14th index\n",
        "\n",
        "embedding_layer.get_weights()[0][14,:]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03453961, -0.02127813, -0.04826909, -0.01245668,  0.02125387,\n",
              "        0.01800379,  0.02056428,  0.03300549, -0.00454269, -0.00098237,\n",
              "        0.04798057, -0.01275452,  0.03132631,  0.02252268, -0.00747663,\n",
              "       -0.00748044], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFuhReOm9xF7"
      },
      "source": [
        "#### Create and apply an `Embedding` layer that uses `mask_zero=True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKYwKT_I-H2O"
      },
      "source": [
        "# Create a layer that uses the mask_zero kwarg\n",
        "\n",
        "mask_embedding_layer = tf.keras.layers.Embedding(input_dim=501, output_dim=16, mask_zero=True)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hc1zx6A-H6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a1a848-c518-4ce8-d484-3662f43f2f6b"
      },
      "source": [
        "# Apply this layer to the sequence and see the _keras_mask property\n",
        "\n",
        "masked_sequence_of_embeddings = mask_embedding_layer(sequence_of_indices)\n",
        "masked_sequence_of_embeddings._keras_mask"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4, 1), dtype=bool, numpy=\n",
              "array([[[False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True]]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUG6LF1MJDL0"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## The Embedding Projector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zna3TCoTAu00"
      },
      "source": [
        "#### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPKYrlepAxPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e356df5-60c9-414b-af45-43c8a7deb901"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBAX9ENDBFFE"
      },
      "source": [
        "#### Load and preprocess the IMDb data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo5qBbDDBIdn"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGpymnb2BIiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128cdc57-b29a-4800-ee95-28ef087c22e8"
      },
      "source": [
        "# Load the dataset\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = get_and_pad_imdb_dataset()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EtU2vK0BLts"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=10000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtkIEIdRBSxv"
      },
      "source": [
        "# Get the word index\n",
        "\n",
        "imdb_word_index = get_imdb_word_index()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur2fp10YBS6T"
      },
      "source": [
        "# Swap the keys and values of the word index\n",
        "\n",
        "inv_imdb_word_index = {value:key for key, value in imdb_word_index.items()}"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cquirCA8BS99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015eee6f-e380-4574-86fd-c3b46e037bc1"
      },
      "source": [
        "# View the first dataset example sentence\n",
        "\n",
        "[inv_imdb_word_index[index] for index in x_train[100] if index>2]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'am',\n",
              " 'a',\n",
              " 'great',\n",
              " 'fan',\n",
              " 'of',\n",
              " 'david',\n",
              " 'lynch',\n",
              " 'and',\n",
              " 'have',\n",
              " 'everything',\n",
              " 'that',\n",
              " \"he's\",\n",
              " 'made',\n",
              " 'on',\n",
              " 'dvd',\n",
              " 'except',\n",
              " 'for',\n",
              " 'hotel',\n",
              " 'room',\n",
              " 'the',\n",
              " '2',\n",
              " 'hour',\n",
              " 'twin',\n",
              " 'peaks',\n",
              " 'movie',\n",
              " 'so',\n",
              " 'when',\n",
              " 'i',\n",
              " 'found',\n",
              " 'out',\n",
              " 'about',\n",
              " 'this',\n",
              " 'i',\n",
              " 'immediately',\n",
              " 'grabbed',\n",
              " 'it',\n",
              " 'and',\n",
              " 'and',\n",
              " 'what',\n",
              " 'is',\n",
              " 'this',\n",
              " \"it's\",\n",
              " 'a',\n",
              " 'bunch',\n",
              " 'of',\n",
              " 'drawn',\n",
              " 'black',\n",
              " 'and',\n",
              " 'white',\n",
              " 'cartoons',\n",
              " 'that',\n",
              " 'are',\n",
              " 'loud',\n",
              " 'and',\n",
              " 'foul',\n",
              " 'mouthed',\n",
              " 'and',\n",
              " 'unfunny',\n",
              " 'maybe',\n",
              " 'i',\n",
              " \"don't\",\n",
              " 'know',\n",
              " \"what's\",\n",
              " 'good',\n",
              " 'but',\n",
              " 'maybe',\n",
              " 'this',\n",
              " 'is',\n",
              " 'just',\n",
              " 'a',\n",
              " 'bunch',\n",
              " 'of',\n",
              " 'crap',\n",
              " 'that',\n",
              " 'was',\n",
              " 'on',\n",
              " 'the',\n",
              " 'public',\n",
              " 'under',\n",
              " 'the',\n",
              " 'name',\n",
              " 'of',\n",
              " 'david',\n",
              " 'lynch',\n",
              " 'to',\n",
              " 'make',\n",
              " 'a',\n",
              " 'few',\n",
              " 'bucks',\n",
              " 'too',\n",
              " 'let',\n",
              " 'me',\n",
              " 'make',\n",
              " 'it',\n",
              " 'clear',\n",
              " 'that',\n",
              " 'i',\n",
              " \"didn't\",\n",
              " 'care',\n",
              " 'about',\n",
              " 'the',\n",
              " 'foul',\n",
              " 'language',\n",
              " 'part',\n",
              " 'but',\n",
              " 'had',\n",
              " 'to',\n",
              " 'keep',\n",
              " 'the',\n",
              " 'sound',\n",
              " 'because',\n",
              " 'my',\n",
              " 'neighbors',\n",
              " 'might',\n",
              " 'have',\n",
              " 'all',\n",
              " 'in',\n",
              " 'all',\n",
              " 'this',\n",
              " 'is',\n",
              " 'a',\n",
              " 'highly',\n",
              " 'disappointing',\n",
              " 'release',\n",
              " 'and',\n",
              " 'may',\n",
              " 'well',\n",
              " 'have',\n",
              " 'just',\n",
              " 'been',\n",
              " 'left',\n",
              " 'in',\n",
              " 'the',\n",
              " 'box',\n",
              " 'set',\n",
              " 'as',\n",
              " 'a',\n",
              " 'curiosity',\n",
              " 'i',\n",
              " 'highly',\n",
              " 'recommend',\n",
              " 'you',\n",
              " \"don't\",\n",
              " 'spend',\n",
              " 'your',\n",
              " 'money',\n",
              " 'on',\n",
              " 'this',\n",
              " '2',\n",
              " 'out',\n",
              " 'of',\n",
              " '10']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrE0rpCVJDL1"
      },
      "source": [
        "#### Build an Embedding layer into a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPUfv9kjJDL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14336b6-117f-4001-8ff0-91f9ad400026"
      },
      "source": [
        "# Get the maximum token value\n",
        "\n",
        "max_index_value = max(imdb_word_index.values())\n",
        "max_index_value"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO0CkecjJDL5"
      },
      "source": [
        "# Specify an embedding dimension\n",
        "\n",
        "embedding_dim = 16"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFBZOlp3JDL7"
      },
      "source": [
        "# Build a model using Sequential:\n",
        "#     1. Embedding layer\n",
        "#     2. GlobalAveragePooling1D\n",
        "#     3. Dense\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(input_dim = max_index_value+1, output_dim=embedding_dim, mask_zero=False),\n",
        "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                             tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw9qhtlPJDL9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "d61e1a23-0752-49e8-c761-570ebf507ec3"
      },
      "source": [
        "# Functional API refresher: use the Model to build the same model\n",
        "\n",
        "review_sequence = tf.keras.Input((None, ))\n",
        "embedding_sequence = tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=embedding_dim,)(review_sequence)\n",
        "average_embedding = tf.keras.layers.GlobalAveragePooling1D()(embedding_sequence),\n",
        "positive_probability = tf.keras.layers.Dense(units=1, activation='sigmoid')(average_embedding)\n",
        "\n",
        "model = tf.keras.Model(inputs = review_sequence, outputs = positive_probability) "
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-ef35a3694bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_index_value\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maverage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpositive_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2710\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2711\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1177\u001b[0m                       'dtype %s' % (dtype,))\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m     \u001b[0mlast_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlast_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \"\"\"\n\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \"\"\"\n\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    204\u001b[0m             TypeError(\"Dimension value must be integer or None or have \"\n\u001b[1;32m    205\u001b[0m                       \u001b[0;34m\"an __index__ method, got value '{0!r}' with type '{1!r}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                       .format(value, type(value))), None)\n\u001b[0m\u001b[1;32m    207\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dimension %d must be >= 0\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got value 'TensorShape([None, 16])' with type '<class 'tensorflow.python.framework.tensor_shape.TensorShape'>'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf6oaEvTCKxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce96e67-d727-4aa1-d800-76896e2aade2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_30 (Embedding)     (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_28  (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrX43gwPJDL-"
      },
      "source": [
        "#### Compile, train, and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVfI_1EoJDL_"
      },
      "source": [
        "# Compile the model with a binary cross-entropy loss\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer='adam')"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvR-O7wGJDMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d43ef0d-2fea-469d-9743-d45cbbb5f6bb"
      },
      "source": [
        "# Train the model using .fit(), savng its history\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test), validation_steps=20)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 9s 8ms/step - loss: 0.6920 - accuracy: 0.5452 - val_loss: 0.6866 - val_accuracy: 0.4875\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6790 - accuracy: 0.6328 - val_loss: 0.6522 - val_accuracy: 0.7047\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6437 - accuracy: 0.7379 - val_loss: 0.6062 - val_accuracy: 0.7656\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.5929 - accuracy: 0.7858 - val_loss: 0.5588 - val_accuracy: 0.7875\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.5453 - accuracy: 0.8151 - val_loss: 0.5159 - val_accuracy: 0.7937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ind0d_gvJDMG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "b66e7605-4321-439d-f88f-58fda65e68ff"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHcNqGnWJDMH"
      },
      "source": [
        "#### The TensorFlow embedding projector\n",
        "\n",
        "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVP4G9X0JDMH"
      },
      "source": [
        "# Retrieve the embedding layer's weights from the trained model\n",
        "\n",
        "weights = model.layers[0].get_weights()[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHd29nfZJDMJ"
      },
      "source": [
        "# Save the word Embeddings to tsv files\n",
        "# Two files: \n",
        "#     one contains the embedding labels (meta.tsv),\n",
        "#     one contains the embeddings (vecs.tsv)\n",
        "\n",
        "import io\n",
        "from os import path\n",
        "\n",
        "out_v = io.open(path.join('/content/drive/MyDrive', 'vecs.tsv'), 'w', encoding='utf-8')\n",
        "out_m = io.open(path.join('/content/drive/MyDrive', 'meta.tsv'), 'w', encoding='utf-8')\n",
        "\n",
        "k = 0\n",
        "\n",
        "for word, token in imdb_word_index.items():\n",
        "    if k != 0:\n",
        "        out_m.write('\\n')\n",
        "        out_v.write('\\n')\n",
        "    \n",
        "    out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
        "    out_m.write(word)\n",
        "    k += 1\n",
        "    \n",
        "out_v.close()\n",
        "out_m.close()\n",
        "# beware large collections of embeddings!"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ti4kMquJDML"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## Recurrent neural network layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hrm6Q2jJDMM"
      },
      "source": [
        "#### Initialize and pass an input to a SimpleRNN layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM7uSwkQJDMR"
      },
      "source": [
        "# Create a SimpleRNN layer and test it\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_YJUyrEJDMT"
      },
      "source": [
        "# Note that only the final cell output is returned\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_rwdZtmJDMV"
      },
      "source": [
        "#### Load and transform the IMDB review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvAtycIpH1aA"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49rJuSFmJDMV"
      },
      "source": [
        "# Load the dataset\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwUHcFKwH4wh"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=10000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfyqmfOXJDMX"
      },
      "source": [
        "# Get the word index using get_imdb_word_index()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR7y1e-xJDMd"
      },
      "source": [
        "#### Create a recurrent neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tym76m2dIVOZ"
      },
      "source": [
        "# Get the maximum index value\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tO953-oJDMd"
      },
      "source": [
        "# Using Sequential, build the model:\n",
        "# 1. Embedding.\n",
        "# 2. LSTM.\n",
        "# 3. Dense.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v076l5CUJDMf"
      },
      "source": [
        "#### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRRXW5mPJDMg"
      },
      "source": [
        "# Compile the model with binary cross-entropy loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "216PeHZFJDMi"
      },
      "source": [
        "# Fit the model and save its training history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NVXF1TSJDMj"
      },
      "source": [
        "#### Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms9VW07lJDMk"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AAyEd6wJDMm"
      },
      "source": [
        "#### Make predictions with the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk9CHYLiJDMo"
      },
      "source": [
        "# View the first test data example sentence\n",
        "# (invert the word index)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blf6in2dJDMs"
      },
      "source": [
        "# Get the model prediction using model.predict()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCyKBsZHJDMt"
      },
      "source": [
        "# Get the corresponding label\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpaI83PlJDMv"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_6\"></a>\n",
        "## Stacked RNNs and the Bidirectional wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qr4kOevKRt8"
      },
      "source": [
        "#### Load and transform the IMDb review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i06LdJiXKRt9"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjA8JlQVKRuB"
      },
      "source": [
        "# Load the dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iBFGx9_KRuD"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=10000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29lcV0UGKRuF"
      },
      "source": [
        "# Get the word index using get_imdb_word_index()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh_Vv9-5JDM1"
      },
      "source": [
        "#### Build stacked and bidirectional recurrent models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Sy-gMyLLEI"
      },
      "source": [
        "# Get the maximum index value and specify an embedding dimension\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89yWIAFdJDM1"
      },
      "source": [
        "# Using Sequential, build a stacked LSTM model via return_sequences=True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-34ZWvRJDM3"
      },
      "source": [
        "# Using Sequential, build a bidirectional RNN with merge_mode='sum'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSpOIOCmJDM4"
      },
      "source": [
        "# Create a model featuring both stacked recurrent layers and a bidirectional layer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3srEhqCJDM7"
      },
      "source": [
        "#### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Dy_C6-JDM7"
      },
      "source": [
        "# Compile the model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er8atiBoJDM9"
      },
      "source": [
        "# Train the model, saving its history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLOLtBKwJDNA"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}